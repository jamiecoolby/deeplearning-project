{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3b4ecaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertJapaneseTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"_name_or_path\": \"cl-tohoku/bert-base-japanese\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"BertJapaneseTokenizer\",\n",
      "  \"transformers_version\": \"4.47.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese\")\n",
    "model = BertModel.from_pretrained(\"cl-tohoku/bert-base-japanese\")\n",
    "\n",
    "\n",
    "print(model.config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1360b899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据条数: 7481\n",
      "第一条数据: {'sentence': 'ホッケーにはデンジャラスプレーの反則があるので、膝より上にボールを浮かすことは基本的に反則になるが、その例外の一つがこのスクープである。', 'tokens': [{'word': 'ホッケー', 'pos': '名詞'}, {'word': 'に', 'pos': '助詞'}, {'word': 'は', 'pos': '助詞'}, {'word': 'デンジャラス', 'pos': '名詞'}, {'word': 'プレー', 'pos': '名詞'}, {'word': 'の', 'pos': '助詞'}, {'word': '反則', 'pos': '名詞'}, {'word': 'が', 'pos': '助詞'}, {'word': 'ある', 'pos': '動詞'}, {'word': 'の', 'pos': '助詞'}, {'word': 'で', 'pos': '助動詞'}, {'word': '、', 'pos': '補助記号'}, {'word': '膝', 'pos': '名詞'}, {'word': 'より', 'pos': '助詞'}, {'word': '上', 'pos': '名詞'}, {'word': 'に', 'pos': '助詞'}, {'word': 'ボール', 'pos': '名詞'}, {'word': 'を', 'pos': '助詞'}, {'word': '浮かす', 'pos': '動詞'}, {'word': 'こと', 'pos': '名詞'}, {'word': 'は', 'pos': '助詞'}, {'word': '基本', 'pos': '名詞'}, {'word': '的', 'pos': '接尾辞'}, {'word': 'に', 'pos': '助動詞'}, {'word': '反則', 'pos': '名詞'}, {'word': 'に', 'pos': '助詞'}, {'word': 'なる', 'pos': '動詞'}, {'word': 'が', 'pos': '助詞'}, {'word': '、', 'pos': '補助記号'}, {'word': 'その', 'pos': '連体詞'}, {'word': '例外', 'pos': '名詞'}, {'word': 'の', 'pos': '助詞'}, {'word': '一', 'pos': '名詞'}, {'word': 'つ', 'pos': '接尾辞'}, {'word': 'が', 'pos': '助詞'}, {'word': 'この', 'pos': '連体詞'}, {'word': 'スクープ', 'pos': '名詞'}, {'word': 'で', 'pos': '助動詞'}, {'word': 'ある', 'pos': '動詞'}, {'word': '。', 'pos': '補助記号'}]}\n",
      "数据条数: 538\n",
      "第一条数据: {'sentence': 'ただし、50周年ソングに変更後は、EDも歌つきのものが使われた。', 'tokens': [{'word': 'ただし', 'pos': '接続詞'}, {'word': '、', 'pos': '補助記号'}, {'word': '50', 'pos': '名詞'}, {'word': '周年', 'pos': '名詞'}, {'word': 'ソング', 'pos': '名詞'}, {'word': 'に', 'pos': '助詞'}, {'word': '変更', 'pos': '名詞'}, {'word': '後', 'pos': '接尾辞'}, {'word': 'は', 'pos': '助詞'}, {'word': '、', 'pos': '補助記号'}, {'word': 'ED', 'pos': '名詞'}, {'word': 'も', 'pos': '助詞'}, {'word': '歌', 'pos': '名詞'}, {'word': 'つき', 'pos': '接尾辞'}, {'word': 'の', 'pos': '助詞'}, {'word': 'もの', 'pos': '名詞'}, {'word': 'が', 'pos': '助詞'}, {'word': '使わ', 'pos': '動詞'}, {'word': 'れ', 'pos': '助動詞'}, {'word': 'た', 'pos': '助動詞'}, {'word': '。', 'pos': '補助記号'}]}\n",
      "数据条数: 572\n",
      "第一条数据: {'sentence': 'これに不快感を示す住民はいましたが,現在,表立って反対や抗議の声を挙げている住民はいないようです。', 'tokens': [{'word': 'これ', 'pos': '代名詞'}, {'word': 'に', 'pos': '助詞'}, {'word': '不快', 'pos': '名詞'}, {'word': '感', 'pos': '名詞'}, {'word': 'を', 'pos': '助詞'}, {'word': '示す', 'pos': '動詞'}, {'word': '住民', 'pos': '名詞'}, {'word': 'は', 'pos': '助詞'}, {'word': 'い', 'pos': '動詞'}, {'word': 'まし', 'pos': '助動詞'}, {'word': 'た', 'pos': '助動詞'}, {'word': 'が', 'pos': '助詞'}, {'word': ',', 'pos': '記号'}, {'word': '現在', 'pos': '名詞'}, {'word': ',', 'pos': '記号'}, {'word': '表立っ', 'pos': '動詞'}, {'word': 'て', 'pos': '助詞'}, {'word': '反対', 'pos': '名詞'}, {'word': 'や', 'pos': '助詞'}, {'word': '抗議', 'pos': '名詞'}, {'word': 'の', 'pos': '助詞'}, {'word': '声', 'pos': '名詞'}, {'word': 'を', 'pos': '助詞'}, {'word': '挙げ', 'pos': '動詞'}, {'word': 'て', 'pos': '助詞'}, {'word': 'いる', 'pos': '動詞'}, {'word': '住民', 'pos': '名詞'}, {'word': 'は', 'pos': '助詞'}, {'word': 'い', 'pos': '動詞'}, {'word': 'ない', 'pos': '助動詞'}, {'word': 'よう', 'pos': '形状詞'}, {'word': 'です', 'pos': '助動詞'}, {'word': '。', 'pos': '補助記号'}]}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "with open(\"output-train.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    data_train = json.load(file)\n",
    "\n",
    "\n",
    "print(f\"nums: {len(data_train)}\")\n",
    "print(f\"firstdata: {data_train[0]}\")\n",
    "\n",
    "with open(\"output-val.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    data_val = json.load(file)\n",
    "\n",
    "\n",
    "print(f\"nums: {len(data_val)}\")\n",
    "print(f\"firstdata: {data_val[0]}\")\n",
    "\n",
    "\n",
    "with open(\"output-test.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    data_test = json.load(file)\n",
    "\n",
    "print(f\"nums: {len(data_test)}\")\n",
    "print(f\"firstdata: {data_test[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8571a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始句子: ホッケーにはデンジャラスプレーの反則があるので、膝より上にボールを浮かすことは基本的に反則になるが、その例外の一つがこのスクープである。\n",
      "分词结果和词性: (['ホッケー', 'に', 'は', 'デンジャラス', 'プレー', 'の', '反則', 'が', 'ある', 'の', 'で', '、', '膝', 'より', '上', 'に', 'ボール', 'を', '浮かす', 'こと', 'は', '基本', '的', 'に', '反則', 'に', 'なる', 'が', '、', 'その', '例外', 'の', '一', 'つ', 'が', 'この', 'スクープ', 'で', 'ある', '。'], ['名詞', '助詞', '助詞', '名詞', '名詞', '助詞', '名詞', '助詞', '動詞', '助詞', '助動詞', '補助記号', '名詞', '助詞', '名詞', '助詞', '名詞', '助詞', '動詞', '名詞', '助詞', '名詞', '接尾辞', '助動詞', '名詞', '助詞', '動詞', '助詞', '補助記号', '連体詞', '名詞', '助詞', '名詞', '接尾辞', '助詞', '連体詞', '名詞', '助動詞', '動詞', '補助記号'])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def preprocess_data(data):\n",
    "    sentences = []\n",
    "    token_labels = []\n",
    "\n",
    "    for item in data:\n",
    "        sentence = item[\"sentence\"]  \n",
    "        tokens = [token[\"word\"] for token in item[\"tokens\"]]  \n",
    "        pos_tags = [token[\"pos\"] for token in item[\"tokens\"]] \n",
    "\n",
    "        sentences.append(sentence)\n",
    "        token_labels.append((tokens, pos_tags))\n",
    "\n",
    "    return sentences, token_labels\n",
    "\n",
    "\n",
    "sentences_train, token_labels_train = preprocess_data(data_train)\n",
    "sentences_val, token_labels_val = preprocess_data(data_val)\n",
    "sentences_test, token_labels_test = preprocess_data(data_test)\n",
    "\n",
    "\n",
    "print(f\"sentence: {sentences_train[0]}\")\n",
    "print(f\"pos: {token_labels_train[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96289d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label to ID mapping: {'代名詞': 1, '副詞': 2, '助動詞': 3, '助詞': 4, '動詞': 5, '名詞': 6, '形容詞': 7, '形状詞': 8, '感動詞': 9, '接尾辞': 10, '接続詞': 11, '接頭辞': 12, '補助記号': 13, '記号': 14, '連体詞': 15}\n"
     ]
    }
   ],
   "source": [
    "def generate_label_to_id(data):\n",
    "    label_set = set()\n",
    "    for item in data:\n",
    "        for token in item[\"tokens\"]:\n",
    "            label_set.add(token[\"pos\"]) \n",
    "\n",
    "    # 创建映射\n",
    "    label_to_id = {label: idx for idx, label in enumerate(sorted(label_set), start=1)}\n",
    "    return label_to_id\n",
    "\n",
    "\n",
    "all_data = data_train + data_val + data_test\n",
    "label_to_id = generate_label_to_id(all_data)\n",
    "\n",
    "\n",
    "print(\"Label to ID mapping:\", label_to_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96a41004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids_train shape: torch.Size([7481, 128])\n",
      "attention_masks_train shape: torch.Size([7481, 128])\n",
      "labels_train shape: torch.Size([7481, 128])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# Function to preprocess data and convert to BERT format\n",
    "def encode_data(data, tokenizer, label_to_id, max_len=128):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    labels = []\n",
    "\n",
    "    for item in data:\n",
    "        # Extract sentences and labels\n",
    "        sentence = item[\"sentence\"]\n",
    "        tokens = [token[\"word\"] for token in item[\"tokens\"]]\n",
    "        pos_tags = [token[\"pos\"] for token in item[\"tokens\"]]\n",
    "\n",
    "        # Tokenize the input sentence\n",
    "        encoding = tokenizer(\n",
    "            sentence,\n",
    "            truncation=True,\n",
    "            max_length=max_len,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt',\n",
    "            is_split_into_words=False  # Tokenize as a whole sentence\n",
    "        )\n",
    "\n",
    "        # Convert labels to IDs\n",
    "        label_ids = [0] * max_len  # Default label for special tokens like [CLS] and [SEP]\n",
    "        for idx, pos in enumerate(pos_tags[:max_len - 2]):  # Exclude special tokens\n",
    "            label_ids[idx + 1] = label_to_id.get(pos, 0)  # Offset by 1 to skip [CLS]\n",
    "\n",
    "        input_ids.append(encoding['input_ids'].squeeze(0))\n",
    "        attention_masks.append(encoding['attention_mask'].squeeze(0))\n",
    "        labels.append(torch.tensor(label_ids))\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    return torch.stack(input_ids), torch.stack(attention_masks), torch.stack(labels)\n",
    "\n",
    "# Convert training, validation, and test datasets\n",
    "input_ids_train, attention_masks_train, labels_train = encode_data(data_train, tokenizer, label_to_id)\n",
    "input_ids_val, attention_masks_val, labels_val = encode_data(data_val, tokenizer, label_to_id)\n",
    "input_ids_test, attention_masks_test, labels_test = encode_data(data_test, tokenizer, label_to_id)\n",
    "\n",
    "# Print the shapes of the converted data\n",
    "print(f\"input_ids_train shape: {input_ids_train.shape}\")\n",
    "print(f\"attention_masks_train shape: {attention_masks_train.shape}\")\n",
    "print(f\"labels_train shape: {labels_train.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ac48fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training batches: 468\n",
      "Number of validation batches: 34\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Create a TensorDataset from the data\n",
    "train_dataset = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n",
    "val_dataset = TensorDataset(input_ids_val, attention_masks_val, labels_val)\n",
    "\n",
    "# Create DataLoader for batching\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Print the number of batches\n",
    "print(f\"Number of training batches: {len(train_loader)}\")\n",
    "print(f\"Number of validation batches: {len(val_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afa3bdcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of frozen parameters: 85\n",
      "BertForTokenClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(32000, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=16, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForTokenClassification\n",
    "from transformers import DistilBertForTokenClassification\n",
    "\n",
    "# Initialize the model\n",
    "# Correctly set num_labels\n",
    "model = BertForTokenClassification.from_pretrained(\n",
    "    \"cl-tohoku/bert-base-japanese\",\n",
    "    num_labels=len(label_to_id) + 1  # Adjust num_labels\n",
    ")\n",
    "for param in model.bert.embeddings.parameters():\n",
    "    param.requires_grad = False  # Freeze embeddings\n",
    "for param in model.bert.encoder.layer[:5].parameters():\n",
    "    param.requires_grad = False  # Freeze first 6 encoder layers\n",
    "\n",
    "# Check frozen parameters\n",
    "frozen_params = sum(1 for param in model.parameters() if not param.requires_grad)\n",
    "print(f\"Number of frozen parameters: {frozen_params}\")\n",
    "# Print the model architecture\n",
    "print(model)\n",
    "model.config.hidden_dropout_prob = 0.3  # Hidden layer dropout\n",
    "#model.config.attention_probs_dropout_prob = 0.3  # Attention dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6f4eb93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label distribution: Counter({6: 60362, 4: 43930, 5: 19169, 13: 16684, 3: 12824, 10: 5950, 7: 1661, 8: 1540, 2: 1488, 12: 1427, 14: 1392, 15: 1015, 1: 903, 11: 667, 9: 51})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "# Add 'PAD' token to label_to_id mapping\n",
    "label_to_id[\"PAD\"] = 0\n",
    "id_to_label = {v: k for k, v in label_to_id.items()}  # Regenerate id_to_label mapping\n",
    "\n",
    "# Count labels in training data\n",
    "label_counts = Counter(label for labels in labels_train.tolist() for label in labels if label != label_to_id[\"PAD\"])\n",
    "print(\"Label distribution:\", label_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed33de24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\ml\\lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(32000, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=16, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Compute class weights\n",
    "class_weights = {label: 1.0 / count for label, count in label_counts.items()}\n",
    "weights = torch.tensor([class_weights.get(i, 1.0) for i in range(len(label_to_id))], device=device)\n",
    "\n",
    "# Weighted loss function\n",
    "loss_fn = torch.nn.CrossEntropyLoss(weight=weights, ignore_index=label_to_id[\"PAD\"])\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=5e-5)\n",
    "\n",
    "\n",
    "# Check if GPU is available and set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97b46cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max label: 15, Num labels: 16\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "# Check if labels contain out-of-range values\n",
    "print(f\"Max label: {labels_train.max()}, Num labels: {len(label_to_id)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7edf64bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training: 100%|████████████████████████████████████████████████| 468/468 [01:02<00:00,  7.53it/s, loss=0.187]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Average Training Loss: 0.3400976394231503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Validation: 100%|████████████████████████████████████████████████| 34/34 [00:02<00:00, 16.57it/s, loss=0.237]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Average Validation Loss: 0.259760312297765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Training: 100%|█████████████████████████████████████████████████| 468/468 [01:04<00:00,  7.27it/s, loss=0.33]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Average Training Loss: 0.24571780526103118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Validation: 100%|████████████████████████████████████████████████| 34/34 [00:01<00:00, 17.60it/s, loss=0.212]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Average Validation Loss: 0.23246542057570288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Training: 100%|█████████████████████████████████████████████████| 468/468 [01:03<00:00,  7.41it/s, loss=0.26]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Average Training Loss: 0.21263613742895615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Validation: 100%|████████████████████████████████████████████████| 34/34 [00:01<00:00, 17.21it/s, loss=0.195]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Average Validation Loss: 0.21220067669363582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Training: 100%|████████████████████████████████████████████████| 468/468 [01:03<00:00,  7.35it/s, loss=0.165]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Average Training Loss: 0.1887203662091086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Validation: 100%|████████████████████████████████████████████████| 34/34 [00:01<00:00, 17.06it/s, loss=0.188]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Average Validation Loss: 0.19899348213392146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Training: 100%|████████████████████████████████████████████████| 468/468 [01:04<00:00,  7.30it/s, loss=0.193]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Average Training Loss: 0.1677382009374535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Validation: 100%|████████████████████████████████████████████████| 34/34 [00:01<00:00, 17.05it/s, loss=0.183]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Average Validation Loss: 0.19382435477831783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "epochs = 5  # Number of epochs\n",
    "for epoch in range(epochs):\n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "\n",
    "    # Training step\n",
    "    train_progress = tqdm(train_loader, desc=f\"Epoch {epoch+1} - Training\")\n",
    "    for batch in train_progress:\n",
    "        input_ids, attention_masks, labels = [x.to(device) for x in batch]\n",
    "\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_masks, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "        train_progress.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1} - Average Training Loss: {avg_train_loss}\")\n",
    "\n",
    "    # Validation step\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "\n",
    "    val_progress = tqdm(val_loader, desc=f\"Epoch {epoch+1} - Validation\")\n",
    "    with torch.no_grad():\n",
    "        for batch in val_progress:\n",
    "            input_ids, attention_masks, labels = [x.to(device) for x in batch]\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_masks, labels=labels)\n",
    "            loss = outputs.loss\n",
    "\n",
    "            total_val_loss += loss.item()\n",
    "            val_progress.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    print(f\"Epoch {epoch+1} - Average Validation Loss: {avg_val_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42383174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict on test data\n",
    "def predict_on_test(model, test_loader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    predictions, true_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids, attention_masks, labels = [x.to(device) for x in batch]\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_masks)\n",
    "            logits = outputs.logits  # Raw scores\n",
    "            preds = torch.argmax(logits, dim=-1)  # Predicted labels\n",
    "\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    return predictions, true_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad8f1795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to decode predictions into human-readable format\n",
    "def decode_predictions(input_ids, predictions, true_labels, tokenizer, id_to_label):\n",
    "    decoded_results = []\n",
    "    for ids, preds, labels in zip(input_ids, predictions, true_labels):\n",
    "        # Convert token IDs to actual tokens\n",
    "        tokens = tokenizer.convert_ids_to_tokens(ids, skip_special_tokens=True)\n",
    "        \n",
    "        # Decode predicted labels and true labels\n",
    "        pred_labels = [id_to_label.get(p, \"UNK\") for p in preds[:len(tokens)]]\n",
    "        true_labels_decoded = [id_to_label.get(t, \"UNK\") for t in labels[:len(tokens)]]\n",
    "\n",
    "        decoded_results.append((tokens, pred_labels, true_labels_decoded))\n",
    "    return decoded_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c0dbc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate accuracy\n",
    "def calculate_accuracy(predictions, true_labels, id_to_label):\n",
    "    total, correct = 0, 0\n",
    "\n",
    "    for preds, labels in zip(predictions, true_labels):\n",
    "        preds = preds.flatten()\n",
    "        labels = labels.flatten()\n",
    "\n",
    "        for pred, label in zip(preds, labels):\n",
    "            if id_to_label[label] != \"PAD\":  # Ignore PAD tokens\n",
    "                total += 1\n",
    "                if pred == label:\n",
    "                    correct += 1\n",
    "\n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b931885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy: 56.41%\n"
     ]
    }
   ],
   "source": [
    "# Add PAD token to label mappings\n",
    "label_to_id[\"PAD\"] = 0\n",
    "id_to_label = {v: k for k, v in label_to_id.items()}\n",
    "\n",
    "# Predict on test data\n",
    "test_loader = DataLoader(TensorDataset(input_ids_test, attention_masks_test, labels_test), batch_size=16)\n",
    "predictions, true_labels = predict_on_test(model, test_loader)\n",
    "\n",
    "# Decode predictions\n",
    "id_to_label = {v: k for k, v in label_to_id.items()}  # Reverse mapping\n",
    "decoded_results = decode_predictions(\n",
    "    input_ids_test.tolist(),\n",
    "    predictions,\n",
    "    true_labels,\n",
    "    tokenizer,\n",
    "    id_to_label\n",
    ")\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = calculate_accuracy(predictions, true_labels, id_to_label)\n",
    "print(f\"Test Set Accuracy: {accuracy:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0070bd84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['[CLS]', 'ホッケー', '##には', '##デン', '##ジャ', '##ラス', '##プレー', '##の', '反', '則', 'が', '##ある', '##の', '##で', '、', '膝', 'より', '上', 'に', '##ボール', '##を', '浮', 'かす', '##こと', '##は', '基', '本', '的', 'に', '反', '則', 'に', '##なる', '##が', '、', 'その', '例', '外', 'の', '一', 'つ', '##が', '##こ', '##の', '##スク', '##ープ', '##で', '##ある', '。', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "Labels: ['PAD', '名詞', '助詞', '助詞', '名詞', '名詞', '助詞', '名詞', '助詞', '動詞', '助詞', '助動詞', '補助記号', '名詞', '助詞', '名詞', '助詞', '名詞', '助詞', '動詞', '名詞', '助詞', '名詞', '接尾辞', '助動詞', '名詞', '助詞', '動詞', '助詞', '補助記号', '連体詞', '名詞', '助詞', '名詞', '接尾辞', '助詞', '連体詞', '名詞', '助動詞', '動詞', '補助記号', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Tokens: ['[CLS]', 'また', '行', 'きた', '##い', '、', 'そんな', '気', '持', 'ち', '##に', '##さ', '##せ', '##て', '##く', '##れる', '##お', '店', 'です', '。', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "Labels: ['PAD', '接続詞', '動詞', '助動詞', '補助記号', '連体詞', '名詞', '助詞', '動詞', '助動詞', '助詞', '動詞', '接頭辞', '名詞', '助動詞', '補助記号', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "Tokens: ['[CLS]', '手', 'に', '持', 'っ', '##た', '特', '殊', 'な', '刃', '物', 'を', '使', 'っ', '##た', '##アク', '##ロ', '##バ', '##ティック', '##な', '体', '術', 'や', '、', '揚', '羽', 'と', '薄', '羽', '同', '様', 'に', '##ク', '##ナイ', '##や', '忍', '具', 'を', '使', 'って', '攻', '撃', 'し', '##て', '##くる', '。', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "Labels: ['PAD', '名詞', '助詞', '動詞', '助動詞', '形状詞', '助動詞', '名詞', '助詞', '動詞', '助動詞', '形状詞', '助動詞', '名詞', '助詞', '補助記号', '名詞', '助詞', '名詞', '形状詞', '助動詞', '名詞', '助詞', '名詞', '名詞', '助詞', '動詞', '助詞', '名詞', '動詞', '助詞', '動詞', '補助記号', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n"
     ]
    }
   ],
   "source": [
    "# Verify the alignment between input tokens and labels\n",
    "for input_id, label in zip(input_ids_train[:3], labels_train[:3]):\n",
    "    print(\"Tokens:\", tokenizer.convert_ids_to_tokens(input_id.tolist()))\n",
    "    print(\"Labels:\", [id_to_label[l] for l in label.tolist()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c616322d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53282625",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
